{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# (0) Import libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection._split import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import validation_curve","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# (1) Data Import & EDA\n\ndata = pd.read_csv(\"../input/top-250-football-transfers-from-2000-to-2018/top250-00-19.csv\")\nprint(data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.loc[data['Age'] < 15])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (2) Data Cleaning\n\n# Fixing age 0 player Marzouq Al-Otaibi to 25 according to TransferMarkt\ndata.loc[data['Age'] == 0, 'Age'] = 25\n\n# Dropping relatively irrelevant columns\nfor drop_columns in ['Name', 'Team_from', 'League_from', 'Team_to', 'League_to']:\n    data = data.drop(drop_columns, axis='columns')\n\n# Redefining Seasons column to beginning year of season\ndata.Season = data.Season.str.slice(start=0, stop=4).astype(int)\n\n# Handling NaN values of Market_value\ndata.Market_value.fillna(data.Transfer_fee, inplace=True)\n\n# Changing positions into separate binary columns for machine learning\npositionsArray = data.Position.unique().astype(str)\ndata = pd.concat((data,pd.get_dummies(data['Position'])),axis=1)\ndata = data.drop('Position', axis='columns')\n\n# Check data\ndata.info()\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (3) Machine Learning: Random Forest Regressor\n\nX = data\ny = data['Transfer_fee']\n\n# Pick test size of 0.5 to avoid overfitting + gives better r^2 and lower rmse than 0.4\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state=42)\nforest = RandomForestRegressor(random_state = 1)\n\n# Performance on Test Set\nforest.fit(X_train, y_train)\ny_pred = forest.predict(X_test)\n\nrmsd = np.sqrt(mean_squared_error(y_test, y_pred))      \nr2_value = r2_score(y_test, y_pred) \n\nprint(rmsd)\nprint(r2_value)\n\n# Performance on Training Set\nforest.fit(X_test, y_test)\ny_pred = forest.predict(X_train)\n\nrmsd = np.sqrt(mean_squared_error(y_train, y_pred))      \nr2_value = r2_score(y_train, y_pred) \n\nprint(rmsd)\nprint(r2_value)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (4) Hyperparameter Tuning\n\n### (4.1) Number of Estimators\nnum_est = np.arange(100, 1000, 50)\ntrain_scoreNum, test_scoreNum = validation_curve(\n                                RandomForestRegressor(),\n                                X = X_train, y = y_train, \n                                param_name = 'n_estimators', \n                                param_range = num_est, cv = 3)\n\n# Calculate mean and standard deviation for training set scores\ntrain_mean = np.mean(train_scoreNum, axis=1)\ntrain_std = np.std(train_scoreNum, axis=1)\n\n# Calculate mean and standard deviation for test set scores\ntest_mean = np.mean(test_scoreNum, axis=1)\ntest_std = np.std(test_scoreNum, axis=1)\n\n# Plot mean accuracy scores for training and test sets\nplt.plot(num_est, train_mean, label=\"Training score\", color=\"black\")\nplt.plot(num_est, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n\nplt.title(\"Validation Curve With Random Forest\")\nplt.xlabel(\"Number Of Trees\")\nplt.ylabel(\"Accuracy Score\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()\n\n## 700 is best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### (4.2) Max Depth\nm_depth = [5, 10, 15, 20, 25, 30]\ntrain_scoreNum, test_scoreNum = validation_curve(\n                                RandomForestRegressor(),\n                                X = X_train, y = y_train, \n                                param_name = 'max_depth', \n                                param_range = m_depth, cv = 3)\n\n# Calculate mean and standard deviation for training set scores\ntrain_mean = np.mean(train_scoreNum, axis=1)\ntrain_std = np.std(train_scoreNum, axis=1)\n\n# Calculate mean and standard deviation for test set scores\ntest_mean = np.mean(test_scoreNum, axis=1)\ntest_std = np.std(test_scoreNum, axis=1)\n\n# Plot mean accuracy scores for training and test sets\nplt.plot(m_depth, train_mean, label=\"Training score\", color=\"black\")\nplt.plot(m_depth, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n\nplt.title(\"Max_Depth\")\nplt.xlabel(\"Number Of Trees\")\nplt.ylabel(\"Accuracy Score\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()\n\n# Pick 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### (4.3) Minimum Samples Split\nm_split = [2, 5, 10, 15, 20, 25, 30]\ntrain_scoreNum, test_scoreNum = validation_curve(\n                                RandomForestRegressor(),\n                                X = X_train, y = y_train, \n                                param_name = 'min_samples_split', \n                                param_range = m_split, cv = 3)\n\n# Calculate mean and standard deviation for training set scores\ntrain_mean = np.mean(train_scoreNum, axis=1)\ntrain_std = np.std(train_scoreNum, axis=1)\n\n# Calculate mean and standard deviation for test set scores\ntest_mean = np.mean(test_scoreNum, axis=1)\ntest_std = np.std(test_scoreNum, axis=1)\n\n# Plot mean accuracy scores for training and test sets\nplt.plot(m_split, train_mean, label=\"Training score\", color=\"black\")\nplt.plot(m_split, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n\nplt.title(\"Validation Curve With Random Forest\")\nplt.xlabel(\"Min_Samples_Split\")\nplt.ylabel(\"Accuracy Score\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()\n\n# Pick 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### (4.4) Minimum Samples Leaf\nm_leaf = [1, 2, 4, 6, 8, 10]\ntrain_scoreNum, test_scoreNum = validation_curve(\n                                RandomForestRegressor(),\n                                X = X_train, y = y_train, \n                                param_name = 'min_samples_leaf', \n                                param_range = m_leaf, cv = 3)\n\n# Calculate mean and standard deviation for training set scores\ntrain_mean = np.mean(train_scoreNum, axis=1)\ntrain_std = np.std(train_scoreNum, axis=1)\n\n# Calculate mean and standard deviation for test set scores\ntest_mean = np.mean(test_scoreNum, axis=1)\ntest_std = np.std(test_scoreNum, axis=1)\n\n# Plot mean accuracy scores for training and test sets\nplt.plot(m_leaf, train_mean, label=\"Training score\", color=\"black\")\nplt.plot(m_leaf, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n\nplt.title(\"Validation Curve With Random Forest\")\nplt.xlabel(\"Min_Samples_Leaf\")\nplt.ylabel(\"Accuracy Score\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()\n\n# Pick 2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (5) Machine Learning With Hyperparameter Tuning: Random Forest Regressor\n\nX = data\ny = data['Transfer_fee']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state=42)\nforest = RandomForestRegressor(random_state = 1, n_estimators = 300, max_depth = 15, min_samples_split= 2, min_samples_leaf = 2)\n\n# Performance on Test Set\nforest.fit(X_train, y_train)\ny_pred = forest.predict(X_test)\n\nrmsd = np.sqrt(mean_squared_error(y_test, y_pred))      \nr2_value = r2_score(y_test, y_pred) \n\nprint(rmsd)\nprint(r2_value)\n\n# Performance on Training Set\nforest.fit(X_test, y_test)\ny_pred = forest.predict(X_train)\n\nrmsd = np.sqrt(mean_squared_error(y_train, y_pred))      \nr2_value = r2_score(y_train, y_pred) \n\nprint(rmsd)\nprint(r2_value)\n\n# Pick pre-tuned model because performance on training set increases after tuning but on test set it decreases, a hint of overfitting","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}